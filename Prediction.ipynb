{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime as dt\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "path = os.path.join(os.getcwd(),\"datasets\",\"Longer\")\n",
    "\n",
    "def files(path, ext=None):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        for file in filenames:\n",
    "            if os.path.isfile(os.path.join(dirpath, file)):\n",
    "                f_dta = None\n",
    "                for file_dta in filenames:\n",
    "                    if file.split(\".\")[0] in file_dta and file != file_dta:\n",
    "                        f_dta = file_dta\n",
    "                \n",
    "                if ext is None:\n",
    "                    yield dirpath, file, f_dta\n",
    "                elif ext in file.split(\".\")[-1]:\n",
    "                    yield dirpath, file, f_dta\n",
    "\n",
    "\n",
    "file_paths = []\n",
    "for file in files(path, \"csv\"):\n",
    "    file_paths.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_from_csv_with_datetime(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'icp' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    t0 = (time[0] - int(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600 - t0\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "\n",
    "    t = np.delete(raw_t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "    fs_hat = round(1/(t[1]-t[0]), -1)\n",
    "\n",
    "    return t, icp, fs_hat\n",
    "\n",
    "\n",
    "def detect_icp_pulses(icp_raw, fs=100):\n",
    "    icp = signal.detrend(icp_raw)\n",
    "    Wc1 = 20 / fs\n",
    "    Wc2 = 0.7 / (fs/2)\n",
    "\n",
    "    b1, a1 = signal.iirfilter(N=8, Wn=Wc1, btype='lowpass', rs=60, rp=1, ftype='cheby1', output='ba')\n",
    "    f_icp = signal.filtfilt(b1, a1, icp)\n",
    "\n",
    "    b2, a2 = signal.iirfilter(N=8, Wn=Wc2, btype='highpass', rs=60, rp=1, ftype='cheby1', output='ba')\n",
    "    f_icp = signal.filtfilt(b2, a2, f_icp)\n",
    "\n",
    "    dICP = np.diff(f_icp)\n",
    "    SSF = np.insert(dICP, 0, 0)\n",
    "    SSF[np.argwhere(SSF < 0)] = 0\n",
    "\n",
    "    z_icp = np.copy(f_icp)\n",
    "    z_icp[np.argwhere(z_icp < 0)] = 0\n",
    "    nSSF = np.multiply(SSF, z_icp)\n",
    "\n",
    "    min_dist = 0.4 * fs\n",
    "    pk_locs = signal.find_peaks(nSSF, distance=min_dist)[0]\n",
    "\n",
    "    po_locs = []\n",
    "    for pk_ind, pk_loc in enumerate(pk_locs[:-1]):\n",
    "        w = f_icp[pk_locs[pk_ind]:pk_locs[pk_ind + 1]]\n",
    "        w_rev = w[::-1]\n",
    "        offset = np.argmin(w_rev)\n",
    "        po_locs.append(pk_locs[pk_ind + 1] - offset - 1)\n",
    "\n",
    "    return f_icp, po_locs\n",
    "\n",
    "# p = Path(r'test.csv')\n",
    "# t, icp, fs = read_from_csv_with_datetime(p)\n",
    "# f_icp, po_locs = detect_icp_pulses(icp, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime as dt\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "is_odenet = True\n",
    "\n",
    "device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "def norm(dim):\n",
    "    return nn.GroupNorm(min(32, dim), dim)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.norm1 = norm(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.norm2 = norm(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.relu(self.norm1(x))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(out)\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out + shortcut\n",
    "\n",
    "\n",
    "class ConcatConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n",
    "        super(ConcatConv2d, self).__init__()\n",
    "        module = nn.ConvTranspose1d if transpose else nn.Conv1d\n",
    "        self._layer = module(\n",
    "            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        tt = torch.ones_like(x[:, :1, :]) * t\n",
    "        ttx = torch.cat([tt, x], 1)\n",
    "        return self._layer(ttx)\n",
    "\n",
    "\n",
    "class ODEfunc(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ODEfunc, self).__init__()\n",
    "        self.norm1 = norm(dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
    "        self.norm2 = norm(dim)\n",
    "        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
    "        self.norm3 = norm(dim)\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        self.nfe += 1\n",
    "        out = self.norm1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(t, out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(t, out)\n",
    "        out = self.norm3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ODEBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, odefunc):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.odefunc = odefunc\n",
    "        self.integration_time = torch.tensor([0, 1]).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.integration_time = self.integration_time.type_as(x)\n",
    "        out = odeint(self.odefunc, x, self.integration_time, rtol=1e-5, atol=1e-5)\n",
    "        return out[1]\n",
    "\n",
    "    @property\n",
    "    def nfe(self):\n",
    "        return self.odefunc.nfe\n",
    "\n",
    "    @nfe.setter\n",
    "    def nfe(self, value):\n",
    "        self.odefunc.nfe = value\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
    "        return x.view(-1, shape)\n",
    "\n",
    "\n",
    "downsampling_layers = [\n",
    "        nn.Conv1d(1, 64, 3, 1),\n",
    "        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
    "        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
    "    ]\n",
    "\n",
    "feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n",
    "fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool1d(1), Flatten(), nn.Dropout(0.6), nn.Linear(64, 5)]\n",
    "\n",
    "model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_batches(pulses, batch_size=2048, sig_length=180):\n",
    "    for i in range(0,len(pulses),batch_size):\n",
    "        tensors = []\n",
    "        for pulse in pulses[i:i+batch_size]:\n",
    "            pulse = np.array(pulse)\n",
    "            if len(pulse) <= sig_length:\n",
    "                bckg = np.zeros(sig_length)\n",
    "                bckg[-len(pulse):] = pulse\n",
    "            else:\n",
    "                middle = int(len(pulse)/2)\n",
    "                bckg = pulse[middle-sig_length/2:middle+sig_lenght/2]\n",
    "            if len(bckg) != sig_length:\n",
    "                print(len(bckg))\n",
    "            tensors.append(bckg)\n",
    "        yield torch.tensor(tensors,dtype=torch.float)\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), \"experiments\", \"2020-03-17_ODE_5cls_1\", \"model_1.pth\")\n",
    "\n",
    "for paths in tqdm(file_paths):\n",
    "    csv_file = read_from_csv_with_datetime(os.path.join(paths[0], paths[1]))\n",
    "    f_icp, po_locs = detect_icp_pulses(csv_file[1], csv_file[2])\n",
    "    pulses = [(f_icp[po_locs[i]: po_locs[i+1]]-min(f_icp[po_locs[i]: po_locs[i+1]]))\n",
    "                  /(max(f_icp[po_locs[i]: po_locs[i+1]])-min(f_icp[po_locs[i]: po_locs[i+1]])) \n",
    "                  for i in range(len(po_locs)-1)]\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'))['state_dict'])\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    for batch in tqdm(get_batches(pulses)):\n",
    "        batch = batch.unsqueeze(1)\n",
    "        outputs = model(batch)\n",
    "        softmax_outputs = F.softmax(outputs, dim=1).tolist()\n",
    "        all_outputs += np.argmax(softmax_outputs, axis=1).tolist()\n",
    "        \n",
    "    diction = {\n",
    "    \"model_predictions\": all_outputs,\n",
    "    \"time\": csv_file[0],\n",
    "    \"icp\": csv_file[1],\n",
    "    \"fs_hat\": csv_file[2],\n",
    "    \"f_icp\": f_icp,\n",
    "    \"po_locs\": po_locs,\n",
    "    \"pulses\": pulses\n",
    "    }\n",
    "    with open(os.path.join(os.getcwd(), \"results\",paths[1].split(\".\")[0]+\".pkl\" ), 'wb') as infile:\n",
    "        loaded = pickle.dump(infile, diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "y = copy.deepcopy(all_outputs)\n",
    "x = [int((po_locs[i]+ po_locs[i+1])/2) for i in range(len(po_locs)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls1 = [1 if i == 0 else 0 for i in y ]\n",
    "cls2 = [1 if i == 1 else 0 for i in y ]\n",
    "cls3 = [1 if i == 2 else 0 for i in y ]\n",
    "cls4 = [1 if i == 3 else 0 for i in y ]\n",
    "cls5 = [1 if i == 4 else 0 for i in y ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [csv_file[0][i] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "diction = {\n",
    "    \"model_predictions\": y,\n",
    "    \"model_idx\": x,\n",
    "    \"model_time\": time,\n",
    "    \"time\": csv_file[0],\n",
    "    \"icp\": csv_file[1],\n",
    "    \"fs_hat\": csv_file[2],\n",
    "    \"f_icp\": f_icp,\n",
    "    \"po_locs\": po_locs,\n",
    "    \"pulses\": pulses\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(os.getcwd(), \"results\",'PAC_1.pkl' ), 'rb') as infile:\n",
    "    loaded = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def amp_from_fft(sig):\n",
    "    windowed = sig * blackmanharris(len(sig))\n",
    "    f = np.fft.fft(windowed)\n",
    "    amplitudes = 2 / len(sig) * np.abs(f)\n",
    "    i = np.argmax(amplitudes)\n",
    "    return amplitudes[i]\n",
    "\n",
    "time_diff = 300*int(loaded[\"fs_hat\"])\n",
    "small_time_diff = 10*int(loaded[\"fs_hat\"])\n",
    "means = []\n",
    "four = []\n",
    "for j in range(0, len(loaded[\"icp\"]), small_time_diff):\n",
    "    means.append(np.mean(loaded[\"icp\"][j:j+small_time_diff]))\n",
    "    four.append(amp_from_fft(loaded[\"icp\"][j:j+small_time_diff]))\n",
    "    \n",
    "raps = []\n",
    "tms = []\n",
    "y = loaded['model_predictions']\n",
    "cls1 = [1 if i == 0 else 0 for i in y ]\n",
    "cls2 = [1 if i == 1 else 0 for i in y ]\n",
    "cls3 = [1 if i == 2 else 0 for i in y ]\n",
    "cls4 = [1 if i == 3 else 0 for i in y ]\n",
    "cls5 = [1 if i == 4 else 0 for i in y ]\n",
    "\n",
    "m1=[]\n",
    "m2=[]\n",
    "m3=[]\n",
    "m4=[]\n",
    "m5=[]\n",
    "po_locs = loaded[\"po_locs\"]\n",
    "model_times = [loaded[\"time\"][int(po_locs[i+1])] for i in range(len(po_locs)-1)]\n",
    "for i in range(30, len(means), 6):\n",
    "    raps.append(pearsonr(means[i-30:i], four[i-30:i])[0])\n",
    "    time_start = loaded[\"time\"][(i-30)*small_time_diff]\n",
    "    time_end = loaded[\"time\"][i*small_time_diff]\n",
    "    times = [1 if time_start<=t<time_end else 0 for t in model_times]\n",
    "    m1.append(sum([cls1[i] if times[i]==1 else 0 for i in range(len(cls1))])/sum(times))\n",
    "    m2.append(sum([cls2[i] if times[i]==1 else 0 for i in range(len(cls2))])/sum(times))\n",
    "    m3.append(sum([cls3[i] if times[i]==1 else 0 for i in range(len(cls3))])/sum(times))\n",
    "    m4.append(sum([cls4[i] if times[i]==1 else 0 for i in range(len(cls4))])/sum(times))\n",
    "    m5.append(sum([cls5[i] if times[i]==1 else 0 for i in range(len(cls5))])/sum(times))\n",
    "    tms.append(time_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=tms, y=raps,\n",
    "                    mode='lines',\n",
    "                    name='Rap'))\n",
    "fig.add_trace(go.Scatter(x=tms, y=m1,\n",
    "                    mode='lines',\n",
    "                    name='T1'))\n",
    "fig.add_trace(go.Scatter(x=tms, y=m2,\n",
    "                    mode='lines',\n",
    "                    name='T2'))\n",
    "fig.add_trace(go.Scatter(x=tms, y=m3,\n",
    "                    mode='lines',\n",
    "                    name='T3'))\n",
    "fig.add_trace(go.Scatter(x=tms, y=m4,\n",
    "                    mode='lines',\n",
    "                    name='T4'))\n",
    "fig.add_trace(go.Scatter(x=tms, y=m5,\n",
    "                    mode='lines',\n",
    "                    name='A+E'))\n",
    "fig.update_layout(title='Changes in Rap value and percentages of predicted classes',\n",
    "                   xaxis_title='Time[seconds]')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris, medfilt, detrend, filtfilt, iirfilter\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "\n",
    "\n",
    "def amp_from_fft(sig, fs):\n",
    "#     windowed = sig * blackmanharris(len(sig))\n",
    "    f = np.fft.fft(sig)\n",
    "    freqs = np.fft.fftfreq(len(sig), 1/fs)\n",
    "    amplitudes = 2 / len(sig) * np.abs(f)\n",
    "#     print(amplitudes)\n",
    "#     input()\n",
    "    amps = [amplitudes[i] if 0.66<=freqs[i]<=3 else 0 for i in range(len(amplitudes)) ]\n",
    "#     amps=amplitudes[1:]\n",
    "    return max(amps)\n",
    "\n",
    "def read_from_csv_with_datetime(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'icp' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    t0 = (time[0] - int(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600 - t0\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "    fs_hat = round(1/(t[1]-t[0]), -1)\n",
    "\n",
    "    return t, icp, fs_hat\n",
    "\n",
    "def read_rap(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'rap' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    \n",
    "    clear_time = np.where(np.isnan(time))\n",
    "    time = np.delete(time, clear_time)\n",
    "#     t0 = (time[0] - np.floor(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600\n",
    "    raw_t = raw_t - raw_t[0]\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "    raw_icp = np.delete(raw_icp, clear_time)\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "\n",
    "    return t, icp\n",
    "\n",
    "\n",
    "def files(path, ext=None, recursive=True):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        if recursive or dirpath==path:\n",
    "            for file in filenames:\n",
    "                if os.path.isfile(os.path.join(dirpath, file)):\n",
    "                    f_dta = None\n",
    "                    for file_dta in filenames:\n",
    "                        if file.split(\".\")[0] in file_dta and file != file_dta:\n",
    "                            f_dta = file_dta                \n",
    "                    if ext is None:\n",
    "                        yield dirpath, file, f_dta\n",
    "                    elif ext in file.split(\".\")[-1]:\n",
    "                        yield dirpath, file, f_dta\n",
    "\n",
    "avl_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\"), \"pkl\", False)]\n",
    "signal_files = [paths for paths in files(os.path.join(os.getcwd(), \"datasets\", \"Longer\"), \"csv\")]\n",
    "rap_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"RAP\", \"RAP\"), \"csv\")]\n",
    "\n",
    "@interact(i = (0, len(avl_files)-1, 1),show_signal = (0, 1, 1))\n",
    "def plot_file(i=82, show_signal=0):\n",
    "    with open(os.path.join(avl_files[i][0], avl_files[i][1]), 'rb') as infile:\n",
    "        loaded = pickle.load(infile)\n",
    "        \n",
    "    for j in range(len(rap_files)):\n",
    "            rap_name = rap_files[j][1].split(\".\")[0].split(\"_\")\n",
    "            if \"r1\" in rap_name:\n",
    "                rap_name = \"_\".join(rap_name[:-1])\n",
    "            else:\n",
    "                rap_name = \"_\".join(rap_name)\n",
    "            \n",
    "            if avl_files[i][1].split(\".\")[0] == rap_name:\n",
    "                time, RAP = read_rap(os.path.join(rap_files[j][0], rap_files[j][1]))\n",
    "                break\n",
    "        \n",
    "    y = loaded[\"all_outputs\"]\n",
    "    tms = loaded[\"time\"]\n",
    "    raps = loaded[\"RAP\"]\n",
    "    m1 = loaded[\"T1\"]\n",
    "    m2 = loaded[\"T2\"]\n",
    "    m3 = loaded[\"T3\"]\n",
    "    m4 = loaded[\"T4\"]\n",
    "    m5 = loaded[\"A+E\"]\n",
    "    tms = [i * (tms[1]-tms[0]) for i in range(len(raps))]\n",
    "    if show_signal == 1:\n",
    "        layout = go.Layout(\n",
    "        yaxis=dict(\n",
    "            domain=[0, 0.5]\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            domain=[0.5, 1]\n",
    "        ))\n",
    "        fig = go.Figure(layout=layout)\n",
    "    else:\n",
    "        fig = go.Figure()\n",
    "#     print(time[-1], tms[-1])\n",
    "    fig.add_trace(go.Scatter(x=time, y=RAP,\n",
    "                                mode='lines',\n",
    "                                name='RAP')) \n",
    "#     fig.add_trace(go.Scatter(x=tms, y=raps,\n",
    "#                         mode='lines',\n",
    "#                         name='Rap'))\n",
    "    fig.add_trace(go.Scatter(x=tms, y=m1,\n",
    "                        mode='lines',\n",
    "                        name='T1'))\n",
    "    fig.add_trace(go.Scatter(x=tms, y=m2,\n",
    "                        mode='lines',\n",
    "                        name='T2'))\n",
    "    fig.add_trace(go.Scatter(x=tms, y=m3,\n",
    "                        mode='lines',\n",
    "                        name='T3'))\n",
    "    fig.add_trace(go.Scatter(x=tms, y=m4,\n",
    "                        mode='lines',\n",
    "                        name='T4'))\n",
    "    fig.add_trace(go.Scatter(x=tms, y=m5,\n",
    "                        mode='lines',\n",
    "                        name='A+E'))\n",
    "    if show_signal == 1:\n",
    "        for j in range(len(signal_files)):\n",
    "            if avl_files[i][1].split(\".\")[0] == signal_files[j][1].split(\".\")[0]:\n",
    "                time, signal, freq = read_from_csv_with_datetime(os.path.join(signal_files[j][0], signal_files[j][1]))\n",
    "        step = int(freq * 60)\n",
    "        time = [i*(time[1]-time[0]) for i in range(len(time))]\n",
    "        signalc = np.convolve(signal, np.ones((step,))/step, mode='valid')\n",
    "        fig.add_trace(go.Scatter(x=time[::step], y=signalc[::step],\n",
    "                        mode='lines',\n",
    "                        name='ICP', yaxis=\"y2\")) \n",
    "\n",
    "    \n",
    "    fig.update_layout(title='Changes in Rap value and percentages of predicted classes for ' + avl_files[i][1].split('.')[0],\n",
    "                       xaxis_title='Time[seconds]')\n",
    "\n",
    "    fig.show()\n",
    "    plotly.offline.plot(fig, filename=os.path.join(os.getcwd(), \"results\", \"graphs\", avl_files[i][1].split('.')[0]+'_bez_okna.html'), auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace(file, pattern, subst):\n",
    "    # Read contents from file as a single string\n",
    "    file_handle = open(file, 'r')\n",
    "    file_string = file_handle.read()\n",
    "    file_handle.close()\n",
    "\n",
    "    # Use RE package to allow for replacement (also allowing for (multiline) REGEX)\n",
    "    file_string = (re.sub(pattern, subst, file_string))\n",
    "\n",
    "    # Write contents to file.\n",
    "    # Using mode 'w' truncates the file.\n",
    "    file_handle = open(file, 'w')\n",
    "    file_handle.write(file_string)\n",
    "    file_handle.close()\n",
    "    \n",
    "rap_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"RAP\", \"RAP\"), \"csv\")]\n",
    "for path in rap_files:\n",
    "    filename = os.path.join(path[0], path[1])\n",
    "    replace(filename, \"DateTime,RAP,Amp,icp\", \"DateTime,RAP,Amp,icp,,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris, medfilt, detrend, filtfilt, iirfilter\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "from tqdm import tqdm\n",
    "\n",
    "def amp_from_fft(sig, fs):\n",
    "#     windowed = sig * blackmanharris(len(sig))\n",
    "    f = np.fft.fft(sig)\n",
    "    freqs = np.fft.fftfreq(len(sig), 1/fs)\n",
    "    amplitudes = 2 / len(sig) * np.abs(f)\n",
    "#     print(amplitudes)\n",
    "#     input()\n",
    "    amps = [amplitudes[i] if 0.66<=freqs[i]<=3 else 0 for i in range(len(amplitudes)) ]\n",
    "#     amps=amplitudes[1:]\n",
    "    return max(amps)\n",
    "\n",
    "def read_rap(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'rap' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    \n",
    "    clear_time = np.where(np.isnan(time))\n",
    "    time = np.delete(time, clear_time)\n",
    "#     t0 = (time[0] - np.floor(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600\n",
    "    raw_t = raw_t - raw_t[0]\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "    raw_icp = np.delete(raw_icp, clear_time)\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "\n",
    "    return t, icp\n",
    "\n",
    "\n",
    "def files(path, ext=None, recursive=True):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        if recursive or dirpath==path:\n",
    "            for file in filenames:\n",
    "                if os.path.isfile(os.path.join(dirpath, file)):\n",
    "                    f_dta = None\n",
    "                    for file_dta in filenames:\n",
    "                        if file.split(\".\")[0] in file_dta and file != file_dta:\n",
    "                            f_dta = file_dta                \n",
    "                    if ext is None:\n",
    "                        yield dirpath, file, f_dta\n",
    "                    elif ext in file.split(\".\")[-1]:\n",
    "                        yield dirpath, file, f_dta\n",
    "\n",
    "def read_from_csv_with_datetime(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'icp' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    t0 = (time[0] - int(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600 - t0\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "\n",
    "    t = np.delete(raw_t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "    fs_hat = round(1/(t[1]-t[0]), -1)\n",
    "\n",
    "    return t, icp, fs_hat\n",
    "\n",
    "\n",
    "def detect_icp_pulses(icp_raw, fs=100):\n",
    "    icp = signal.detrend(icp_raw)\n",
    "    Wc1 = 20 / fs\n",
    "    Wc2 = 0.7 / (fs/2)\n",
    "\n",
    "    b1, a1 = signal.iirfilter(N=8, Wn=Wc1, btype='lowpass', rs=60, rp=1, ftype='cheby1', output='ba')\n",
    "    f_icp = signal.filtfilt(b1, a1, icp)\n",
    "\n",
    "    b2, a2 = signal.iirfilter(N=8, Wn=Wc2, btype='highpass', rs=60, rp=1, ftype='cheby1', output='ba')\n",
    "    f_icp = signal.filtfilt(b2, a2, f_icp)\n",
    "\n",
    "    dICP = np.diff(f_icp)\n",
    "    SSF = np.insert(dICP, 0, 0)\n",
    "    SSF[np.argwhere(SSF < 0)] = 0\n",
    "\n",
    "    z_icp = np.copy(f_icp)\n",
    "    z_icp[np.argwhere(z_icp < 0)] = 0\n",
    "    nSSF = np.multiply(SSF, z_icp)\n",
    "\n",
    "    min_dist = 0.4 * fs\n",
    "    pk_locs = signal.find_peaks(nSSF, distance=min_dist)[0]\n",
    "\n",
    "    po_locs = []\n",
    "    for pk_ind, pk_loc in enumerate(pk_locs[:-1]):\n",
    "        w = f_icp[pk_locs[pk_ind]:pk_locs[pk_ind + 1]]\n",
    "        w_rev = w[::-1]\n",
    "        offset = np.argmin(w_rev)\n",
    "        po_locs.append(pk_locs[pk_ind + 1] - offset - 1)\n",
    "\n",
    "    return f_icp, po_locs\n",
    "\n",
    "# p = Path(r'test.csv')\n",
    "# t, icp, fs = read_from_csv_with_datetime(p)\n",
    "# f_icp, po_locs = detect_icp_pulses(icp, fs)\n",
    "\n",
    "avl_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"CNN\"), \"pkl\", False)]\n",
    "signal_files = [paths for paths in files(os.path.join(os.getcwd(), \"datasets\", \"Longer\"), \"csv\")]\n",
    "rap_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"RAP\", \"RAP\"), \"csv\")]\n",
    "\n",
    "\n",
    "# @interact(i = (0, len(avl_files)-1, 1))\n",
    "def plot_file(i=23):\n",
    "#     print(avl_files[i][1])\n",
    "    with open(os.path.join(avl_files[i][0], avl_files[i][1]), 'rb') as infile:\n",
    "        loaded = pickle.load(infile)\n",
    "    with open(os.path.join(avl_files[i][0], \"times\", avl_files[i][1]), 'rb') as infile:\n",
    "        times = pickle.load(infile)\n",
    "    \n",
    "    for j in range(len(rap_files)):\n",
    "            rap_name = rap_files[j][1].split(\".\")[0].split(\"_\")\n",
    "            if \"r1\" in rap_name:\n",
    "                rap_name = \"_\".join(rap_name[:-1])\n",
    "            else:\n",
    "                rap_name = \"_\".join(rap_name)\n",
    "            \n",
    "            if avl_files[i][1].split(\".\")[0] == rap_name:\n",
    "                time, RAP = read_rap(os.path.join(rap_files[j][0], rap_files[j][1]))\n",
    "                break\n",
    "        \n",
    "    for j in range(len(signal_files)):\n",
    "            if avl_files[i][1].split(\".\")[0] == signal_files[j][1].split(\".\")[0]:\n",
    "                time_signal, signal, freq = read_from_csv_with_datetime(os.path.join(signal_files[j][0], signal_files[j][1]))\n",
    "#                 f_icp, po_locs = detect_icp_pulses(signal, freq)\n",
    "#                 print(len(po_locs))\n",
    "                break\n",
    "                \n",
    "    y = loaded[\"all_outputs\"]\n",
    "    data_over = []\n",
    "    data_under = []\n",
    "    \n",
    "    model_times = times[\"model_times\"]\n",
    "    jmp = freq*60\n",
    "    idx = 0\n",
    "    for j in range(len(time)):\n",
    "        time_start = time_signal[idx]\n",
    "        while(time_signal[idx] < time[j] and idx<=len(time_signal)-2):\n",
    "            idx+=1\n",
    "        time_end = time_signal[idx-1]\n",
    "        data = [y[t] for t in range(len(model_times)) if time_start<=model_times[t]<time_end ]\n",
    "        if RAP[j] >= 0.6:\n",
    "            data_over += data\n",
    "        else:\n",
    "            data_under += data\n",
    "                \n",
    "    data_over += [0, 1, 2, 3, 4]\n",
    "    data_under += [0, 1, 2, 3, 4]\n",
    "    \n",
    "    counts_over = np.unique(data_over, return_counts=True)\n",
    "    counts_under = np.unique(data_under, return_counts=True)\n",
    "    \n",
    "    counts_under = [count - 1 for count in counts_under[1]]\n",
    "    counts_over = [count - 1 for count in counts_over[1]]\n",
    "    \n",
    "    labels = [\"T1\", \"T2\", \"T3\", \"T4\", \"A+E\"]\n",
    "    print(counts_under)\n",
    "#     layout = go.Layout(\n",
    "#     yaxis=dict(\n",
    "#         domain=[0, 0.5]\n",
    "#     ),\n",
    "#     yaxis2=dict(\n",
    "#         domain=[0.5, 1]\n",
    "#     ))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "            x=labels,\n",
    "            y=counts_over, name=\"RAP>=0.6\"))\n",
    "    fig.add_trace(go.Bar(x=labels, y=counts_under, name=\"RAP<0.6\"))\n",
    "    \n",
    "    fig.update_layout(title='Histogramy dla chwilowego RAP dla przebiegu ' + avl_files[i][1].split('.')[0],\n",
    "                       xaxis_title='Klasa', yaxis_title='Liczba wykryć')\n",
    "\n",
    "#     fig.show()\n",
    "    plotly.offline.plot(fig, filename=os.path.join(os.getcwd(), \"results\", \"graphs\", \"wszyscyCNN\", avl_files[i][1].split('.')[0]+'_Histogram.html'), auto_open=False)\n",
    "    \n",
    "for i in tqdm(range(0,len(avl_files))):\n",
    "    if i != 86:\n",
    "        plot_file(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_files[j][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris, medfilt, detrend, filtfilt, iirfilter\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_from_csv_with_datetime(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'icp' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    t0 = (time[0] - int(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600 - t0\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "    fs_hat = round(1/(t[1]-t[0]), -1)\n",
    "\n",
    "    return t, icp, fs_hat\n",
    "\n",
    "def read_rap(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'rap' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    \n",
    "    clear_time = np.where(np.isnan(time))\n",
    "    time = np.delete(time, clear_time)\n",
    "#     t0 = (time[0] - np.floor(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600\n",
    "    raw_t = raw_t - raw_t[0]\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "    raw_icp = np.delete(raw_icp, clear_time)\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "\n",
    "    return t, icp\n",
    "\n",
    "\n",
    "def files(path, ext=None, recursive=True):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        if recursive or dirpath==path:\n",
    "            for file in filenames:\n",
    "                if os.path.isfile(os.path.join(dirpath, file)):\n",
    "                    f_dta = None\n",
    "                    for file_dta in filenames:\n",
    "                        if file.split(\".\")[0] in file_dta and file != file_dta:\n",
    "                            f_dta = file_dta                \n",
    "                    if ext is None:\n",
    "                        yield dirpath, file, f_dta\n",
    "                    elif ext in file.split(\".\")[-1]:\n",
    "                        yield dirpath, file, f_dta\n",
    "\n",
    "avl_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"ODE\"), \"pkl\", False)]\n",
    "signal_files = [paths for paths in files(os.path.join(os.getcwd(), \"datasets\", \"Longer\"), \"csv\")]\n",
    "rap_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"RAP\", \"RAP\"), \"csv\")]\n",
    "\n",
    "full_counts_over = np.array([0, 0, 0, 0, 0])\n",
    "full_counts_under = np.array([0, 0, 0, 0, 0])\n",
    "labels = [\"T1\", \"T2\", \"T3\", \"T4\", \"T5\"]\n",
    "\n",
    "for i in tqdm(range(len(avl_files))):\n",
    "    with open(os.path.join(avl_files[i][0], avl_files[i][1]), 'rb') as infile:\n",
    "        loaded = pickle.load(infile)\n",
    "        \n",
    "    for j in range(len(rap_files)):\n",
    "            rap_name = rap_files[j][1].split(\".\")[0].split(\"_\")\n",
    "            if \"r1\" in rap_name:\n",
    "                rap_name = \"_\".join(rap_name[:-1])\n",
    "            else:\n",
    "                rap_name = \"_\".join(rap_name)\n",
    "            \n",
    "            if avl_files[i][1].split(\".\")[0] == rap_name:\n",
    "                time, RAP = read_rap(os.path.join(rap_files[j][0], rap_files[j][1]))\n",
    "                break\n",
    "        \n",
    "    y = loaded[\"all_outputs\"]\n",
    "    RAP_mean = np.mean(RAP)\n",
    "    counts = np.unique(y, return_counts=True)\n",
    "    if RAP_mean >= 0.6:\n",
    "        for j in range(len(counts[0])):\n",
    "            full_counts_over[counts[0][j]] += counts[1][j]\n",
    "    else:\n",
    "        for j in range(len(counts[0])):\n",
    "            full_counts_under[counts[0][j]] += counts[1][j]\n",
    "\n",
    "# layout = go.Layout(\n",
    "#     yaxis=dict(\n",
    "#         domain=[0, 0.5]\n",
    "#     ),\n",
    "#     yaxis2=dict(\n",
    "#         domain=[0.5, 1]\n",
    "#     ))\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "            x=labels,\n",
    "            y=full_counts_over/sum(full_counts_over), name=\"RAP>=0.6\"))\n",
    "fig.add_trace(go.Bar(x=labels, y=full_counts_under/sum(full_counts_under), name=\"RAP<0.6\"))\n",
    "    \n",
    "fig.update_layout(title='Histogram',\n",
    "                       xaxis_title='Class', yaxis_title='Percent of examples')\n",
    "\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename=os.path.join(os.getcwd(), \"results\", \"graphs\",\"CNN\", 'Histogram_2_pelny_procenty.html'), auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris, medfilt, detrend, filtfilt, iirfilter\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# hist_data = [x1, x2, x3, x4]\n",
    "\n",
    "# group_labels = ['Group 1', 'Group 2', 'Group 3', 'Group 4']\n",
    "\n",
    "# # Create distplot with custom bin_size\n",
    "# fig = ff.create_distplot(hist_data, group_labels, bin_size=[.1, .25, .5, 1])\n",
    "# fig.show()\n",
    "def read_from_csv_with_datetime(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'icp' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    t0 = (time[0] - int(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600 - t0\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "    fs_hat = round(1/(t[1]-t[0]), -1)\n",
    "\n",
    "    return t, icp, fs_hat\n",
    "\n",
    "def read_rap(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'rap' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    \n",
    "    clear_time = np.where(np.isnan(time))\n",
    "    time = np.delete(time, clear_time)\n",
    "#     t0 = (time[0] - np.floor(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600\n",
    "    raw_t = raw_t - raw_t[0]\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "    raw_icp = np.delete(raw_icp, clear_time)\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "\n",
    "    return t, icp\n",
    "\n",
    "\n",
    "def files(path, ext=None, recursive=True):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        if recursive or dirpath==path:\n",
    "            for file in filenames:\n",
    "                if os.path.isfile(os.path.join(dirpath, file)):\n",
    "                    f_dta = None\n",
    "                    for file_dta in filenames:\n",
    "                        if file.split(\".\")[0] in file_dta and file != file_dta:\n",
    "                            f_dta = file_dta                \n",
    "                    if ext is None:\n",
    "                        yield dirpath, file, f_dta\n",
    "                    elif ext in file.split(\".\")[-1]:\n",
    "                        yield dirpath, file, f_dta\n",
    "\n",
    "avl_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\"), \"pkl\", False)]\n",
    "signal_files = [paths for paths in files(os.path.join(os.getcwd(), \"datasets\", \"Longer\"), \"csv\")]\n",
    "rap_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"RAP\", \"RAP\"), \"csv\")]\n",
    "\n",
    "full_counts = []\n",
    "raps = []\n",
    "labels = [\"T1\", \"T2\", \"T3\", \"T4\", \"A+E\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "# fig = make_subplots(rows=1, cols=5, specs=[[{}, {}, {}, {}, {}]], shared_xaxes=True,\n",
    "#                     shared_yaxes=True, vertical_spacing=0.001)\n",
    "\n",
    "patients = []\n",
    "for i in tqdm(range(len(avl_files))):\n",
    "    with open(os.path.join(avl_files[i][0], avl_files[i][1]), 'rb') as infile:\n",
    "        loaded = pickle.load(infile)\n",
    "    \n",
    "    for j in range(len(rap_files)):\n",
    "            rap_name = rap_files[j][1].split(\".\")[0].split(\"_\")\n",
    "            if \"r1\" in rap_name:\n",
    "                rap_name = \"_\".join(rap_name[:-1])\n",
    "            else:\n",
    "                rap_name = \"_\".join(rap_name)\n",
    "            \n",
    "            if avl_files[i][1].split(\".\")[0] == rap_name:\n",
    "                time, RAP = read_rap(os.path.join(rap_files[j][0], rap_files[j][1]))\n",
    "                break\n",
    "        \n",
    "    \n",
    "    y = loaded[\"all_outputs\"]\n",
    "    if len(RAP > 0):\n",
    "        RAP_mean = np.mean(RAP)\n",
    "        raps.append(RAP_mean)\n",
    "        y = y + [0, 1, 2, 3, 4]\n",
    "        counts = np.unique(y, return_counts=True)\n",
    "        real_counts = [count - 1 for count in counts[1]]\n",
    "        percent_counts = real_counts/sum(real_counts)\n",
    "        full_counts.append(percent_counts)\n",
    "#         full_counts.append(real_counts)\n",
    "        patients.append(avl_files[i][1])\n",
    "    \n",
    "classes = [[\"T1\", \"T2\", \"T3\", \"T4\", \"A+E\"] for i in range(len(raps))]\n",
    "classes = [item for sublist in classes for item in sublist]\n",
    "\n",
    "patients = [[patient, patient, patient, patient, patient] for patient in patients]\n",
    "patients = [item for sublist in patients for item in sublist]\n",
    "\n",
    "raps = [[rap, rap, rap, rap, rap] for rap in raps]\n",
    "raps = [item for sublist in raps for item in sublist]\n",
    "total_counts =[[np.sum(item), np.sum(item), np.sum(item), np.sum(item), np.sum(item)] for item in full_counts]\n",
    "total_counts =[item for sublist in total_counts for item in sublist] \n",
    "full_counts = [item for sublist in full_counts for item in sublist]\n",
    "print(np.max(full_counts))\n",
    "print(len(raps), len(full_counts), len(classes), len(total_counts), len(patients))\n",
    "diction = {\n",
    "    \"RAP\": raps,\n",
    "    \"size\": full_counts,\n",
    "    \"class\": classes,\n",
    "    \"sum\": total_counts,\n",
    "    \"patients\": patients\n",
    "}\n",
    "df = pd.DataFrame(diction)\n",
    "\n",
    "\n",
    "# for class_idx in range(len(labels)):\n",
    "#     bins = 10\n",
    "#     x = []\n",
    "#     bars = []\n",
    "#     for i in range(bins):\n",
    "#         rap_min = i/bins\n",
    "#         rap_max = (i+1)/bins\n",
    "# #         print(rap_min, rap_max)\n",
    "# #         print(df[df[(df[\"RAP\"] >= rap_min) & (df[\"RAP\"] < rap_max) & (df[\"class\"] == labels[class_idx])]][\"size\"])\n",
    "#         filtered = df[(df[\"RAP\"] >= rap_min) & (df[\"RAP\"] < rap_max)]\n",
    "#         filtered = filtered[df[\"class\"] == labels[class_idx]]\n",
    "# #         bars.append(np.sum(filtered[\"size\"]))\n",
    "#         bars.append(np.sum(filtered[\"size\"])/np.sum(filtered[\"sum\"]))\n",
    "# #         bars.append(np.sum(df[df[(df[\"RAP\"] >= rap_min) & (df[\"RAP\"] < rap_max) & (df[\"class\"] == labels[class_idx])]][\"size\"]))\n",
    "#         x.append((rap_max + rap_min)/2)\n",
    "#     print(bars)\n",
    "#     fig.add_trace(go.Bar(\n",
    "#     y=x,\n",
    "#     x=bars,\n",
    "#     name=labels[class_idx],\n",
    "#     orientation='h',\n",
    "#     ))\n",
    "    \n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"class\"], y=df[\"RAP\"], marker=go.scatter.Marker(size=df[\"size\"], sizeref=1/50), mode=\"markers\",\n",
    "                         customdata=df[\"patients\"],\n",
    "                        hovertemplate=\n",
    "                                    \"<b>%{x}</b><br>\" +\n",
    "                                    \"RAP: %{y:,.2f}<br>\" +\n",
    "                                    \"Percent: %{marker.size:.2%}<br>\" +\n",
    "                                    \"Patient: %{customdata}\"\n",
    "                                    \"<extra></extra>\"))\n",
    "# for class_idx in range(len(labels)):\n",
    "#     class_count = [cnt[class_idx] for cnt in full_counts]\n",
    "#     fig.add_trace(go.Scatter(x=df[\"class\"][df[\"class\"]==labels[class_idx]],\n",
    "#                             y0=df[\"RAP\"][df[\"class\"]==labels[class_idx]],\n",
    "#                             y=df[\"size\"][df[\"class\"]==labels[class_idx]],\n",
    "#                             name=labels[class_idx],\n",
    "#                             meanline_visible=True,\n",
    "#                            spanmode=\"hard\"),1,class_idx+1)\n",
    "fig.update_layout(title='Histograms of detected classes according to mean RAP',\n",
    "                       xaxis_title='Percent of detected examples of class', yaxis_title='Mean RAP')\n",
    "\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename=os.path.join(os.getcwd(), \"results\", \"graphs\", 'Histogram-klasowy_procent.html'), auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris, medfilt, detrend, filtfilt, iirfilter\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import signal\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def amp_from_fft(sig, fs):\n",
    "#     windowed = sig * blackmanharris(len(sig))\n",
    "    f = np.fft.fft(sig)\n",
    "    freqs = np.fft.fftfreq(len(sig), 1/fs)\n",
    "    amplitudes = 2 / len(sig) * np.abs(f)\n",
    "#     print(amplitudes)\n",
    "#     input()\n",
    "    amps = [amplitudes[i] if 0.66<=freqs[i]<=3 else 0 for i in range(len(amplitudes)) ]\n",
    "#     amps=amplitudes[1:]\n",
    "    return max(amps)\n",
    "\n",
    "def read_from_csv_with_datetime(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'icp' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    t0 = (time[0] - int(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600 - t0\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "    fs_hat = round(1/(t[1]-t[0]), -1)\n",
    "\n",
    "    return t, icp, fs_hat\n",
    "\n",
    "def read_rap(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'rap' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    \n",
    "    clear_time = np.where(np.isnan(time))\n",
    "    time = np.delete(time, clear_time)\n",
    "#     t0 = (time[0] - np.floor(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600\n",
    "    raw_t = raw_t - raw_t[0]\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "    raw_icp = np.delete(raw_icp, clear_time)\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "\n",
    "    return t, icp\n",
    "\n",
    "\n",
    "def files(path, ext=None, recursive=True):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        if recursive or dirpath==path:\n",
    "            for file in filenames:\n",
    "                if os.path.isfile(os.path.join(dirpath, file)):\n",
    "                    f_dta = None\n",
    "                    for file_dta in filenames:\n",
    "                        if file.split(\".\")[0] in file_dta and file != file_dta:\n",
    "                            f_dta = file_dta                \n",
    "                    if ext is None:\n",
    "                        yield dirpath, file, f_dta\n",
    "                    elif ext in file.split(\".\")[-1]:\n",
    "                        yield dirpath, file, f_dta\n",
    "                        \n",
    "def detect_icp_pulses(icp_raw, fs=100):\n",
    "    icp = signal.detrend(icp_raw)\n",
    "    Wc1 = 20 / fs\n",
    "    Wc2 = 0.7 / (fs/2)\n",
    "\n",
    "    b1, a1 = signal.iirfilter(N=8, Wn=Wc1, btype='lowpass', rs=60, rp=1, ftype='cheby1', output='ba')\n",
    "    f_icp = signal.filtfilt(b1, a1, icp)\n",
    "\n",
    "    b2, a2 = signal.iirfilter(N=8, Wn=Wc2, btype='highpass', rs=60, rp=1, ftype='cheby1', output='ba')\n",
    "    f_icp = signal.filtfilt(b2, a2, f_icp)\n",
    "\n",
    "    dICP = np.diff(f_icp)\n",
    "    SSF = np.insert(dICP, 0, 0)\n",
    "    SSF[np.argwhere(SSF < 0)] = 0\n",
    "\n",
    "    z_icp = np.copy(f_icp)\n",
    "    z_icp[np.argwhere(z_icp < 0)] = 0\n",
    "    nSSF = np.multiply(SSF, z_icp)\n",
    "\n",
    "    min_dist = 0.4 * fs\n",
    "    pk_locs = signal.find_peaks(nSSF, distance=min_dist)[0]\n",
    "\n",
    "    po_locs = []\n",
    "    for pk_ind, pk_loc in enumerate(pk_locs[:-1]):\n",
    "        w = f_icp[pk_locs[pk_ind]:pk_locs[pk_ind + 1]]\n",
    "        w_rev = w[::-1]\n",
    "        offset = np.argmin(w_rev)\n",
    "        po_locs.append(pk_locs[pk_ind + 1] - offset - 1)\n",
    "\n",
    "    return f_icp, po_locs\n",
    "\n",
    "avl_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\"), \"pkl\", False)]\n",
    "signal_files = [paths for paths in files(os.path.join(os.getcwd(), \"datasets\", \"Longer\"), \"csv\")]\n",
    "rap_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"RAP\", \"RAP\"), \"csv\")]\n",
    "\n",
    "\n",
    "classes = [[],[],[],[],[]]\n",
    "patients = [[],[],[],[],[]]\n",
    "for i in tqdm(range(len(avl_files))):\n",
    "    with open(os.path.join(avl_files[i][0], \"times\", avl_files[i][1]), 'rb') as infile:\n",
    "        times = pickle.load(infile)\n",
    "    \n",
    "    with open(os.path.join(avl_files[i][0], avl_files[i][1]), 'rb') as infile:\n",
    "        loaded = pickle.load(infile)\n",
    "        \n",
    "    for j in range(len(rap_files)):\n",
    "            rap_name = rap_files[j][1].split(\".\")[0].split(\"_\")\n",
    "            if \"r1\" in rap_name:\n",
    "                rap_name = \"_\".join(rap_name[:-1])\n",
    "            else:\n",
    "                rap_name = \"_\".join(rap_name)\n",
    "            \n",
    "            if avl_files[i][1].split(\".\")[0] == rap_name:\n",
    "                time, RAP = read_rap(os.path.join(rap_files[j][0], rap_files[j][1]))\n",
    "                break\n",
    "        \n",
    "    for j in range(len(signal_files)):\n",
    "            if avl_files[i][1].split(\".\")[0] == signal_files[j][1].split(\".\")[0]:\n",
    "                time_signal, signals, freq = read_from_csv_with_datetime(os.path.join(signal_files[j][0], signal_files[j][1]))\n",
    "                f_icp, po_locs = detect_icp_pulses(signals, freq)\n",
    "                break\n",
    "                \n",
    "    y = loaded[\"all_outputs\"]\n",
    "    print(np.unique(y))\n",
    "    break\n",
    "    data_over = []\n",
    "    data_under = []\n",
    "    model_times = times[\"model_times\"]\n",
    "    jmp = freq*60\n",
    "    idx = 0\n",
    "    for j in range(len(time)):\n",
    "        time_start = time_signal[idx]\n",
    "        while(time_signal[idx] < time[j] and idx<=len(time_signal)-2):\n",
    "            idx+=1\n",
    "        time_end = time_signal[idx-1]\n",
    "        data = [y[t] for t in range(len(model_times)-1) if time_start<=model_times[t]<time_end ]\n",
    "        for d in data:\n",
    "            classes[d].append(RAP[j])\n",
    "            patients[d].append(\"\".join(avl_files[i][1].split(\"_\")[:2]))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diction = {\n",
    "    \"classes\": classes,\n",
    "    \"patients\": patients,\n",
    "    \"labels\": [\"T1\", \"T2\", \"T3\", \"T4\", \"A+E\"]\n",
    "    }\n",
    "with open(os.path.join(os.getcwd(), \"results\",\"chwilowe.pkl\" ), 'wb') as infile:\n",
    "     loaded = pickle.dump(diction, infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris, medfilt, detrend, filtfilt, iirfilter\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import signal\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(os.path.join(os.getcwd(), \"results\",\"chwilowe\",\"chwilowe.pkl\" ), 'rb') as infile:\n",
    "     loaded = pickle.load(infile)\n",
    "classes = loaded[\"classes\"]\n",
    "patients = loaded[\"patients\"]\n",
    "len(classes[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    4: 4\n",
    "}\n",
    "data_over = []\n",
    "data_under = []\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes[i])):\n",
    "        if classes[i][j] >= 0.6:\n",
    "            data_over.append(class_dict[i])\n",
    "        else:\n",
    "            data_under.append(class_dict[i])\n",
    "            \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_over = [class_dict[i] for j in range(len(classes[i])) for i in range(len(classes)) if classes[i,j] >= 0.6]\n",
    "# counts_under = [class_dict[i] for j in range(len(classes[i])) for i in range(len(classes)) if classes[i,j] < 0.6]\n",
    "print(counts_over)\n",
    "counts_over = np.unique(data_over, return_counts=True)\n",
    "counts_under = np.unique(data_under, return_counts=True)\n",
    "    \n",
    "counts_under = [count - 1 for count in counts_under[1]]\n",
    "counts_over = [count - 1 for count in counts_over[1]]\n",
    "    \n",
    "labels = [\"T1\", \"T2\", \"T3\", \"T4\", \"A+E\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "            x=labels,\n",
    "            y=counts_over, name=\"RAP>=0.6\"))\n",
    "fig.add_trace(go.Bar(x=labels, y=counts_under, name=\"RAP<0.6\"))\n",
    "    \n",
    "fig.update_layout(title='Histogramy dla chwilowego RAP',\n",
    "                       xaxis_title='Klasa', yaxis_title='Liczba wykryć')\n",
    "\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename=os.path.join(os.getcwd(), \"results\", \"graphs\", 'Chwilowy_Histogram.html'), auto_open=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "            x=labels,\n",
    "            y=counts_over/sum(counts_over), name=\"RAP>=0.6\"))\n",
    "fig.add_trace(go.Bar(x=labels, y=counts_under/sum(counts_under), name=\"RAP<0.6\"))\n",
    "    \n",
    "fig.update_layout(title='Histogram dla RAP chwilowego',\n",
    "                       xaxis_title='Klasa', yaxis_title='Procent klas')\n",
    "\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename=os.path.join(os.getcwd(), \"results\", \"graphs\", 'Histogram_2_pelny_procenty.html'), auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"T1\", \"T2\", \"T3\", \"T4\", \"A+E\"]\n",
    "fig = go.Figure()\n",
    "bars = [[], [], [], [], []]\n",
    "for class_idx in range(len(classes)):\n",
    "    bins = 20\n",
    "    x = []\n",
    "    for i in range(bins):\n",
    "        rap_min = i/bins * 2 - 1\n",
    "        rap_max = (i+1)/bins * 2 - 1\n",
    "#         print(rap_min, rap_max)\n",
    "#         print(df[df[(df[\"RAP\"] >= rap_min) & (df[\"RAP\"] < rap_max) & (df[\"class\"] == labels[class_idx])]][\"size\"])\n",
    "        bar = [1 for val in classes[class_idx] if rap_min <= val < rap_max]\n",
    "#         bars.append(np.sum(bar))\n",
    "        bars[class_idx].append(np.sum(bar))\n",
    "#         bars.append(np.sum(filtered[\"size\"])/np.sum(filtered[\"sum\"]))\n",
    "#         bars.append(np.sum(df[df[(df[\"RAP\"] >= rap_min) & (df[\"RAP\"] < rap_max) & (df[\"class\"] == labels[class_idx])]][\"size\"]))\n",
    "        x.append((rap_max + rap_min)/2)\n",
    "#     print(len(bars[class_idx]))\n",
    "\n",
    "bars = np.array(bars, dtype=np.float32)\n",
    "for bar_idx in range(len(bars[0])):\n",
    "#     print(bar_idx)\n",
    "#     print(bars[:,bar_idx])\n",
    "    bars[:,bar_idx] = bars[:,bar_idx] / np.sum(bars[:,bar_idx])\n",
    "#     print(bars[:,bar_idx])\n",
    "    \n",
    "for class_idx in range(len(classes)):\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=x,\n",
    "        x=bars[class_idx],\n",
    "        name=labels[class_idx],\n",
    "        orientation='h',\n",
    "    ))\n",
    "fig.update_layout(title='Histogram klasowy dla RAP chwilowego',\n",
    "                       xaxis_title='Klasa', yaxis_title='Procent klasy w danym przedziale')\n",
    "\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename=os.path.join(os.getcwd(), \"results\", \"graphs\", 'histogram_klasowy_procent.html'), auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "            x=labels,\n",
    "            y=counts_over/sum(counts_over), name=\"RAP>=0.6\"))\n",
    "fig.add_trace(go.Bar(x=labels, y=counts_under/sum(counts_under), name=\"RAP<0.6\"))\n",
    "    \n",
    "fig.update_layout(title='Histogram dla RAP chwilowego',\n",
    "                       xaxis_title='Klasa', yaxis_title='Procent klas')\n",
    "\n",
    "fig.show()\n",
    "plotly.offline.plot(fig, filename=os.path.join(os.getcwd(), \"results\", \"graphs\", 'Histogram_2_pelny_procenty.html'), auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris, medfilt, detrend, filtfilt, iirfilter\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_from_csv_with_datetime(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'icp' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    t0 = (time[0] - int(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600 - t0\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "    fs_hat = round(1/(t[1]-t[0]), -1)\n",
    "\n",
    "    return t, icp, fs_hat\n",
    "\n",
    "def read_rap(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'rap' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    \n",
    "    clear_time = np.where(np.isnan(time))\n",
    "    time = np.delete(time, clear_time)\n",
    "#     t0 = (time[0] - np.floor(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600\n",
    "    raw_t = raw_t - raw_t[0]\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "    raw_icp = np.delete(raw_icp, clear_time)\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "\n",
    "    return t, icp\n",
    "\n",
    "\n",
    "def files(path, ext=None, recursive=True):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        if recursive or dirpath==path:\n",
    "            for file in filenames:\n",
    "                if os.path.isfile(os.path.join(dirpath, file)):\n",
    "                    f_dta = None\n",
    "                    for file_dta in filenames:\n",
    "                        if file.split(\".\")[0] in file_dta and file != file_dta:\n",
    "                            f_dta = file_dta                \n",
    "                    if ext is None:\n",
    "                        yield dirpath, file, f_dta\n",
    "                    elif ext in file.split(\".\")[-1]:\n",
    "                        yield dirpath, file, f_dta\n",
    "\n",
    "avl_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"ODE\"), \"pkl\", False)]\n",
    "signal_files = [paths for paths in files(os.path.join(os.getcwd(), \"datasets\", \"Longer\"), \"csv\")]\n",
    "rap_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"RAP\", \"RAP\"), \"csv\")]\n",
    "\n",
    "full_counts_over = np.array([0, 0, 0, 0, 0])\n",
    "full_counts_under = np.array([0, 0, 0, 0, 0])\n",
    "labels = [\"T1\", \"T2\", \"T3\", \"T4\", \"T5\"]\n",
    "patient_dictionary = {}\n",
    "for file in avl_files:\n",
    "    match = re.search(r\"PAC[0-9]*\", file[1])\n",
    "    patient_dictionary[match.group(0)] = {\n",
    "        \"T1\": 0,\n",
    "        \"T2\": 0,\n",
    "        \"T3\": 0,\n",
    "        \"T4\": 0,\n",
    "        \"A+E\": 0,\n",
    "        \"RAP\": [],\n",
    "        \"ICP\": []\n",
    "    }\n",
    "    \n",
    "    \n",
    "for i in tqdm(range(len(avl_files))):\n",
    "    patient_name = re.search(r\"PAC[0-9]*\", avl_files[i][1]).group(0)\n",
    "    RAP = None\n",
    "    sig = None\n",
    "    with open(os.path.join(avl_files[i][0], avl_files[i][1]), 'rb') as infile:\n",
    "        loaded = pickle.load(infile)\n",
    "        \n",
    "    for j in range(len(rap_files)):\n",
    "            rap_name = rap_files[j][1].split(\".\")[0].split(\"_\")\n",
    "            if \"r1\" in rap_name:\n",
    "                rap_name = \"_\".join(rap_name[:-1])\n",
    "            else:\n",
    "                rap_name = \"_\".join(rap_name)\n",
    "            \n",
    "            if avl_files[i][1].split(\".\")[0] == rap_name:\n",
    "                time, RAP = read_rap(os.path.join(rap_files[j][0], rap_files[j][1]))\n",
    "                break\n",
    "    \n",
    "    for j in range(len(signal_files)):\n",
    "        if avl_files[i][1].split(\".\")[0] == signal_files[j][1].split(\".\")[0]:\n",
    "                time_sig, sig, f = read_from_csv_with_datetime(os.path.join(signal_files[j][0], signal_files[j][1]))\n",
    "                break\n",
    "    if RAP is not None and sig is not None:\n",
    "        patient_dictionary[patient_name][\"RAP\"] += list(RAP)\n",
    "        patient_dictionary[patient_name][\"ICP\"] += list(sig)\n",
    "    else:\n",
    "        print(\"Could not find files for {}. RAP: {}, SIG: {}\".format(avl_files[i][1], RAP is not None, sig is not None))\n",
    "        continue\n",
    "        \n",
    "    y = loaded[\"all_outputs\"]\n",
    "    bins, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    for j in range(len(bins)):\n",
    "        if 0 <= bins[j] <= 3:\n",
    "            name = \"T{}\".format(bins[j]+1)\n",
    "        else:\n",
    "            name = \"A+E\"\n",
    "        patient_dictionary[patient_name][name] += counts[j]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris, medfilt, detrend, filtfilt, iirfilter\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_from_csv_with_datetime(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'icp' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    t0 = (time[0] - int(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600 - t0\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "    fs_hat = round(1/(t[1]-t[0]), -1)\n",
    "\n",
    "    return t, icp, fs_hat\n",
    "\n",
    "def read_rap(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'rap' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    \n",
    "    clear_time = np.where(np.isnan(time))\n",
    "    time = np.delete(time, clear_time)\n",
    "#     t0 = (time[0] - np.floor(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600\n",
    "    raw_t = raw_t - raw_t[0]\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "    raw_icp = np.delete(raw_icp, clear_time)\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "\n",
    "    return t, icp\n",
    "\n",
    "\n",
    "def files(path, ext=None, recursive=True):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        if recursive or dirpath==path:\n",
    "            for file in filenames:\n",
    "                if os.path.isfile(os.path.join(dirpath, file)):\n",
    "                    f_dta = None\n",
    "                    for file_dta in filenames:\n",
    "                        if file.split(\".\")[0] in file_dta and file != file_dta:\n",
    "                            f_dta = file_dta                \n",
    "                    if ext is None:\n",
    "                        yield dirpath, file, f_dta\n",
    "                    elif ext in file.split(\".\")[-1]:\n",
    "                        yield dirpath, file, f_dta\n",
    "\n",
    "avl_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"ODE\"), \"pkl\", False)]\n",
    "signal_files = [paths for paths in files(os.path.join(os.getcwd(), \"datasets\", \"Longer\"), \"csv\")]\n",
    "rap_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"RAP\", \"RAP\"), \"csv\")]\n",
    "\n",
    "full_counts_over = np.array([0, 0, 0, 0, 0])\n",
    "full_counts_under = np.array([0, 0, 0, 0, 0])\n",
    "labels = [\"T1\", \"T2\", \"T3\", \"T4\", \"T5\"]\n",
    "for file in avl_files:\n",
    "    match = re.search(r\"PAC[0-9]*\", file[1])\n",
    "    patient_dictionary[match.group(0)][\"A+E\"] = 0\n",
    "    \n",
    "for i in tqdm(range(len(avl_files))):\n",
    "    patient_name = re.search(r\"PAC[0-9]*\", avl_files[i][1]).group(0)\n",
    "    RAP = None\n",
    "    sig = None\n",
    "    with open(os.path.join(avl_files[i][0], avl_files[i][1]), 'rb') as infile:\n",
    "        loaded = pickle.load(infile)    \n",
    "    y = loaded[\"all_outputs\"]\n",
    "    bins, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    for j in range(len(bins)):\n",
    "        if 0 <= bins[j] <= 3:\n",
    "            name = \"T{}\".format(bins[j]+1)\n",
    "        else:\n",
    "            name = \"A+E\"\n",
    "            patient_dictionary[patient_name][name] += counts[j]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_dict = {\n",
    "    \"Pac\": [],\n",
    "    \"T1\": [],\n",
    "    \"T2\": [],\n",
    "    \"T3\": [],\n",
    "    \"T4\": [],\n",
    "    \"A+E\": [],\n",
    "    \"RAP_mean\": [],\n",
    "    \"RAP_median\": [],\n",
    "    \"ICP_mean\": [],\n",
    "    \"ICP_median\": []\n",
    "}\n",
    "\n",
    "for patient in tqdm(patient_dictionary.keys()):\n",
    "    df_dict[\"Pac\"].append(patient)\n",
    "    df_dict[\"T1\"].append(patient_dictionary[patient][\"T1\"])\n",
    "    df_dict[\"T2\"].append(patient_dictionary[patient][\"T2\"])\n",
    "    df_dict[\"T3\"].append(patient_dictionary[patient][\"T3\"])\n",
    "    df_dict[\"T4\"].append(patient_dictionary[patient][\"T4\"])\n",
    "    df_dict[\"A+E\"].append(patient_dictionary[patient][\"A+E\"])\n",
    "    df_dict[\"RAP_mean\"].append(np.mean(patient_dictionary[patient][\"RAP\"]))\n",
    "    df_dict[\"RAP_median\"].append(np.median(patient_dictionary[patient][\"RAP\"]))\n",
    "    df_dict[\"ICP_mean\"].append(np.mean(patient_dictionary[patient][\"ICP\"]))\n",
    "    df_dict[\"ICP_median\"].append(np.median(patient_dictionary[patient][\"ICP\"]))\n",
    "\n",
    "df = pd.DataFrame(df_dict).to_csv(os.path.join(os.getcwd(), \"results\", \"CSV\", \"ODE\", \"dane.csv\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_dictionary[\"PAC1\"][\"A+E\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import blackmanharris, medfilt, detrend, filtfilt, iirfilter\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_from_csv_with_datetime(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'icp' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    t0 = (time[0] - int(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600 - t0\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "    fs_hat = round(1/(t[1]-t[0]), -1)\n",
    "\n",
    "    return t, icp, fs_hat\n",
    "\n",
    "def read_rap(file_path):\n",
    "    with open(file_path) as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        headers = next(reader)\n",
    "        hd_time = [hd for hd in headers if 'datetime' in hd.lower()]\n",
    "        hd_icp = [hd for hd in headers if 'rap' in hd.lower()]\n",
    "    with open(file_path) as csv_file:\n",
    "        raw_data = pd.read_csv(csv_file, sep=',', usecols=[hd_time[0], hd_icp[0]])\n",
    "\n",
    "    time = raw_data[hd_time].to_numpy()\n",
    "    \n",
    "    clear_time = np.where(np.isnan(time))\n",
    "    time = np.delete(time, clear_time)\n",
    "#     t0 = (time[0] - np.floor(time[0])) * 24 * 3600\n",
    "    raw_t = (time - np.floor(time)) * 24 * 3600\n",
    "    raw_t = raw_t - raw_t[0]\n",
    "    raw_icp = raw_data[hd_icp].to_numpy()\n",
    "    raw_icp = np.delete(raw_icp, clear_time)\n",
    "    t = [i*(raw_t[1]-raw_t[0]) for i in range(len(raw_t))]\n",
    "    t = np.delete(t, np.where(np.isnan(raw_icp)))\n",
    "    icp = np.delete(raw_icp, np.where(np.isnan(raw_icp)))\n",
    "\n",
    "    return t, icp\n",
    "\n",
    "\n",
    "def files(path, ext=None, recursive=True):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        if recursive or dirpath==path:\n",
    "            for file in filenames:\n",
    "                if os.path.isfile(os.path.join(dirpath, file)):\n",
    "                    f_dta = None\n",
    "                    for file_dta in filenames:\n",
    "                        if file.split(\".\")[0] in file_dta and file != file_dta:\n",
    "                            f_dta = file_dta                \n",
    "                    if ext is None:\n",
    "                        yield dirpath, file, f_dta\n",
    "                    elif ext in file.split(\".\")[-1]:\n",
    "                        yield dirpath, file, f_dta\n",
    "\n",
    "avl_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"ODE\"), \"pkl\", False)]\n",
    "signal_files = [paths for paths in files(os.path.join(os.getcwd(), \"datasets\", \"Longer\"), \"csv\")]\n",
    "rap_files = [paths for paths in files(os.path.join(os.getcwd(), \"results\", \"RAP\", \"RAP\"), \"csv\")]\n",
    "\n",
    "labels = [\"T1\", \"T2\", \"T3\", \"T4\", \"A+E\"]  \n",
    "\n",
    "    \n",
    "for i in tqdm(range(68,len(avl_files))):\n",
    "    with open(os.path.join(avl_files[i][0], \"times\", avl_files[i][1]), 'rb') as infile:\n",
    "        times = pickle.load(infile)\n",
    "    \n",
    "    with open(os.path.join(avl_files[i][0], avl_files[i][1]), 'rb') as infile:\n",
    "        loaded = pickle.load(infile)\n",
    "        \n",
    "    for j in range(len(signal_files)):\n",
    "        if avl_files[i][1].split(\".\")[0] == signal_files[j][1].split(\".\")[0]:\n",
    "                time_sig, sig, f = read_from_csv_with_datetime(os.path.join(signal_files[j][0], signal_files[j][1]))\n",
    "                break\n",
    "    po_locs = times[\"po_locs\"]\n",
    "    m_times = times[\"model_times\"]\n",
    "    y = loaded[\"all_outputs\"]\n",
    "#     print(np.unique(y))\n",
    "    shapes = []\n",
    "    colors = {\n",
    "        0: \"green\",\n",
    "        1: \"blue\",\n",
    "        2: \"red\",\n",
    "        3: \"magenta\",\n",
    "        4: \"goldenrod\"\n",
    "    }\n",
    "#     for j in range(len(po_locs)-1):\n",
    "#         shapes.append(\n",
    "#         dict(\n",
    "#             type=\"rect\",\n",
    "#             # x-reference is assigned to the x-values\n",
    "#             xref=\"x\",\n",
    "#             # y-reference is assigned to the plot paper [0,1]\n",
    "#             yref=\"paper\",\n",
    "#             x0=time_sig[po_locs[j]],\n",
    "#             y0=0,\n",
    "#             x1=time_sig[po_locs[j+1]],\n",
    "#             y1=1,\n",
    "#             fillcolor=colors[y[j]],\n",
    "#             opacity=0.4,\n",
    "#             layer=\"below\",\n",
    "#             line_width=1,\n",
    "#         ))\n",
    "    fig = go.Figure()\n",
    "    po_sigs = [] \n",
    "    po_tms = []\n",
    "    for j in range(len(po_locs)-1):\n",
    "        middle = (po_locs[j] + po_locs[j+1])/2\n",
    "        if middle == np.floor(middle):\n",
    "            po_sigs.append(sig[int(middle)])\n",
    "            po_tms.append(time_sig[int(middle)])\n",
    "        else:\n",
    "            po_sigs.append((sig[int(np.floor(middle))]+sig[int(np.ceil(middle))])/2)\n",
    "            po_tms.append((time_sig[int(np.floor(middle))]+time_sig[int(np.ceil(middle))])/2)\n",
    "#         fig.add_trace(go.Scattergl(x=time_sig[po_locs[j]:po_locs[j+1]], y=sig[po_locs[j]:po_locs[j+1]],\n",
    "#                                     mode='lines',\n",
    "#                                     name='ICP - '+labels[y[j]],\n",
    "#                                     line = dict(\n",
    "#                                     color=colors[y[j]]\n",
    "#                                     )\n",
    "#                                     ))\n",
    "    fig.add_trace(go.Scattergl(x = [time_sig[loc] for loc in po_locs], y = [sig[loc] for loc in po_locs], mode=\"markers\", name='Splits', marker_symbol = \"line-ns\"\n",
    "                               ,marker_line_width=2 , marker_color=\"black\"))\n",
    "    for class_id in range(5):\n",
    "        x_plot = [po_tms[j] for j in range(len(y)) if y[j] == class_id]\n",
    "        y_plot = [po_sigs[j] for j in range(len(y)) if y[j] == class_id]\n",
    "        fig.add_trace(go.Scattergl(x=x_plot, y = y_plot, mode='markers', name=labels[class_id]))\n",
    "    \n",
    "    fig.add_trace(go.Scattergl(x=time_sig, y=sig,\n",
    "                                mode='lines',\n",
    "                                name='ICP'))\n",
    "    \n",
    "#     fig.update_layout(shapes=shapes)\n",
    "    plotly.offline.plot(fig, filename=os.path.join(os.getcwd(), \"results\", \"graphs\",\"klasyfikacje\", avl_files[i][1] + '_przebieg.html'), auto_open=False)\n",
    "#     break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
